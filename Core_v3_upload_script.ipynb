{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc42d8-28a2-4a06-b293-40b7d4013787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae2b957-57b3-4b8e-bafd-eae017183fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from step_2_ticker_converter import ticker_converter\n",
    "from step_3_cloud_clean_data import cloud_clean_data\n",
    "from step_1_historical_index import historical_data, get_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf79ddf-31f6-4a18-ac5e-9fe1329d2160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import re\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "def cloud_clean_data_gcs(bucket_name, folder_prefix):\n",
    "    # Initialize client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # List blobs in the folder\n",
    "    blobs = bucket.list_blobs(prefix=folder_prefix)\n",
    "\n",
    "    rows = []\n",
    "    for blob in blobs:\n",
    "        filename = blob.name.split(\"/\")[-1]\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        if \"10-Q\" not in filename and \"10-K\" not in filename:\n",
    "            continue\n",
    "\n",
    "        # Extract CIK and filing type\n",
    "        cik_match = re.search(r'edgar_data_(\\d+)_', filename)\n",
    "        type_match = re.search(r'10-[QK]', filename)\n",
    "        cik = cik_match.group(1) if cik_match else None\n",
    "        filing_type = type_match.group(0) if type_match else None\n",
    "\n",
    "        # Read first part of the blob (up to 5000 bytes)\n",
    "        try:\n",
    "            content = blob.download_as_bytes(start=0, end=4999).decode('utf-8', errors='ignore')\n",
    "            period_match = re.search(r'CONFORMED PERIOD OF REPORT:\\s*(\\d{8})', content)\n",
    "            period = period_match.group(1) if period_match else None\n",
    "        except Exception as e:\n",
    "            period = None\n",
    "\n",
    "        rows.append({\n",
    "            'cik': cik,\n",
    "            'filing_type': filing_type,\n",
    "            'filename': filename,\n",
    "            'conformed_period_of_report': period,\n",
    "            'bucket_file_path': f\"gs://{bucket_name}/{blob.name}\"\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e529f4c-de7f-4553-a666-aaf43388a0de",
   "metadata": {},
   "source": [
    "Below script did not work for q4 have to add mannually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71bcda-69a5-4744-aaed-b131b7456130",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"sentiment_chloe-curtis\"\n",
    "quarters = [f\"{year}q{q}\" for year in range(2019, 2025) for q in range(1, 5)]\n",
    "dataframes = []\n",
    "\n",
    "for quarter in quarters:\n",
    "    try:\n",
    "        print(f\"Processing {quarter}...\")\n",
    "        df = cloud_clean_data_gcs(bucket_name, f\"{quarter}/\")\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {quarter}: {e}\")\n",
    "\n",
    "cloud_combined = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4948472-3ccd-4534-a813-6a0ef80fe80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CLEANS FUNCTION 3 ##################\n",
    "# Convert to datetime if not already\n",
    "cloud_combined['conformed_period_of_report'] = pd.to_datetime(cloud_combined['conformed_period_of_report'])\n",
    "\n",
    "# Create the 'year' column\n",
    "cloud_combined['year'] = cloud_combined['conformed_period_of_report'].dt.year\n",
    "\n",
    "# Create the 'quarter' column in the format 'Q1', 'Q2', etc.\n",
    "cloud_combined['quarter'] = 'Q' + cloud_combined['conformed_period_of_report'].dt.quarter.astype(str)\n",
    "cloud_combined.loc[:, 'cik'] = cloud_combined['cik'].astype(str).str.zfill(10)\n",
    "\n",
    "cloud_combined['bucket_file_path'] = cloud_combined['bucket_file_path'].str.replace(\n",
    "    'gs://sentiment_chloe-curtis/', 'clean_data_', regex=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f500cc2-c8b8-40c1-810e-4cc07034e07c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T08:24:18.516855Z",
     "iopub.status.busy": "2025-06-10T08:24:18.516475Z",
     "iopub.status.idle": "2025-06-10T08:24:18.526932Z",
     "shell.execute_reply": "2025-06-10T08:24:18.525207Z",
     "shell.execute_reply.started": "2025-06-10T08:24:18.516832Z"
    }
   },
   "source": [
    "Below script gets 2024 Q4, cleans and adds columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c75617-1914-4ab1-92b9-4a0af33baee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024q4_clean =  cloud_clean_data_gcs(\"sentiment_chloe-curtis\", \"clean_data_2024q4/\")\n",
    "\n",
    "df_2024q4_clean['conformed_period_of_report'] = pd.to_datetime(df_2024q4_clean['conformed_period_of_report'])\n",
    "\n",
    "# Create the 'year' column\n",
    "df_2024q4_clean['year'] = df_2024q4_clean['conformed_period_of_report'].dt.year\n",
    "\n",
    "# Create the 'quarter' column in the format 'Q1', 'Q2', etc.\n",
    "df_2024q4_clean['quarter'] = 'Q' + df_2024q4_clean['conformed_period_of_report'].dt.quarter.astype(str)\n",
    "\n",
    "# Ensure 'cik' is a 10-character string with leading zeros\n",
    "df_2024q4_clean.loc[:, 'cik'] = df_2024q4_clean['cik'].astype(str).str.zfill(10)\n",
    "\n",
    "# Modify 'bucket_file_path'\n",
    "df_2024q4_clean['bucket_file_path'] = df_2024q4_clean['bucket_file_path'].str.replace(\n",
    "    'gs://sentiment_chloe-curtis/', '', regex=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed129f5-ee9b-4973-9339-d7872745d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### STEP 1\n",
    "file_path = \"historical_data.txt\"\n",
    "historical_index = historical_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61fe80-b5aa-415f-8631-9bae19fb36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### STEP 2\n",
    "ticker_path = \"ticker_converter.json\"\n",
    "ticker_conversion = ticker_converter(ticker_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4202b5d9-ba6e-4ac5-b9b9-06f06735092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## JOIN STEP 1 & 2 ###################\n",
    "base_index = pd.merge(historical_index, ticker_conversion, on = \"ticker\", how = \"left\")\n",
    "### JOIN CAUSED NAS - AS TICKER CONVERSION DIDNT HAVE COMPANY NAME AND CIK\n",
    "### REMOVE THE NAS FROM THE JOIN\n",
    "x = base_index.dropna()\n",
    "## CREATE DF WITH THE NAS\n",
    "y = base_index[base_index.isnull().any(axis=1)]\n",
    "## DROP THE COLUMNS WE DONT NEED FROM NA DF\n",
    "g = y[['quarter','ticker']]\n",
    "## CSV HAS THE DATA WE NEED\n",
    "missing_company = pd.read_csv('missing_companies.csv')\n",
    "### MERGE BACK TO HAVE THE CIX AND COMPANY NAME \n",
    "z = pd.merge(g, missing_company, on = \"ticker\", how = \"left\")\n",
    "### PAD THE CIK NUMBERS WITH ZEROS - 10 CHARCTERS\n",
    "z['cik'] = z['cik'].astype(str).str.zfill(10)\n",
    "## CONCAT BOTH DFS\n",
    "df = pd.concat([x, z], axis=0, ignore_index=True)\n",
    "# Split the 'quarter' column into two parts\n",
    "df[['quarter', 'year']] = df['quarter'].str.split('-', expand=True)\n",
    "\n",
    "# Convert 2-digit year to 4-digit\n",
    "df['year'] = df['year'].apply(lambda x: '20' + x if int(x) < 50 else '19' + x)\n",
    "df['year'] = df['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06220517-0f04-4e35-898b-babbcdee3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## JOIN STEP 3 WITH 1 & 2\n",
    "core_v3 =pd.merge(df, df_2024q4_clean, on =['cik','quarter','year'], how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0192215e-f8cb-4ace-912b-e4951b9f21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "def upload_core_v3_to_bq_(df):\n",
    "    \"\"\"\n",
    "    Uploads parsed MDA data from EDGAR filings to BigQuery.\n",
    "    Expects columns: cik, filename, management_discussion\n",
    "    \"\"\"\n",
    "    # Rename columns to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Keep only the required columns\n",
    "\n",
    "        BQ_PROJECT_ID = 'sentiment-lewagon'\n",
    "        BQ_DATASET_ID = 'sentiment_db'\n",
    "        BQ_TABLE_ID = 'core_v3'\n",
    "        table_ref = f\"{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_ID}\"\n",
    "\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=\"WRITE_APPEND\"\n",
    "        )\n",
    "\n",
    "        job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "        job.result()\n",
    "\n",
    "        print(f\"✅ Uploaded {job.output_rows} rows to {table_ref}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to upload DataFrame to BigQuery: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514362a4-c676-4066-9f83-6ec18233e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_core_v3_to_bq_(core_v3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
