{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b9d852-f43d-48e3-846a-cecaa5012ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ecae9-0566-45b7-9d97-cb1179de94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from step_2_ticker_converter import ticker_converter\n",
    "from step_3_cloud_clean_data import cloud_clean_data\n",
    "from step_1_historical_index import historical_data, get_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e18e2d-d948-4747-9876-e6f0378e1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"historical_data.txt\"\n",
    "historical_index = historical_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b567b7-543b-44fc-b4ac-4a51ea3aad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_path = \"ticker_converter.json\"\n",
    "ticker_conversion = ticker_converter(ticker_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ae68c-2818-47b9-9711-c8db9c456f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index = pd.merge(historical_index, ticker_conversion, on = \"ticker\", how = \"left\")\n",
    "### JOIN CAUSED NAS - AS TICKER CONVERSION DIDNT HAVE COMPANY NAME AND CIK\n",
    "### REMOVE THE NAS FROM THE JOIN\n",
    "x = base_index.dropna()\n",
    "## CREATE DF WITH THE NAS\n",
    "y = base_index[base_index.isnull().any(axis=1)]\n",
    "## DROP THE COLUMNS WE DONT NEED FROM NA DF\n",
    "g = y[['quarter','ticker']]\n",
    "## CSV HAS THE DATA WE NEED\n",
    "missing_company = pd.read_csv('missing_companies.csv')\n",
    "### MERGE BACK TO HAVE THE CIX AND COMPANY NAME \n",
    "z = pd.merge(g, missing_company, on = \"ticker\", how = \"left\")\n",
    "### PAD THE CIK NUMBERS WITH ZEROS - 10 CHARCTERS\n",
    "z['cik'] = z['cik'].astype(str).str.zfill(10)\n",
    "## CONCAT BOTH DFS\n",
    "df = pd.concat([x, z], axis=0, ignore_index=True)\n",
    "# Split the 'quarter' column into two parts\n",
    "df[['quarter', 'year']] = df['quarter'].str.split('-', expand=True)\n",
    "\n",
    "# Convert 2-digit year to 4-digit\n",
    "df['year'] = df['year'].apply(lambda x: '20' + x if int(x) < 50 else '19' + x)\n",
    "df['year'] = df['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03173f0c-323a-4dba-8660-713dd0fb6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### testing on q1 \n",
    "cloud_2022q1 = cloud_clean_data(\"2022q1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be77f22c-9acd-460c-af08-7dd3467f9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###  CLEANS DATA THIS CAN BE ADDED TO FUCNTIOM\n",
    "\n",
    "\n",
    "# Convert to datetime if not already\n",
    "cloud_2022q1['conformed_period_of_report'] = pd.to_datetime(cloud_2022q1['conformed_period_of_report'])\n",
    "\n",
    "# Create the 'year' column\n",
    "cloud_2022q1['year'] = cloud_2022q1['conformed_period_of_report'].dt.year\n",
    "\n",
    "# Create the 'quarter' column in the format 'Q1', 'Q2', etc.\n",
    "cloud_2022q1['quarter'] = 'Q' + cloud_2022q1['conformed_period_of_report'].dt.quarter.astype(str)\n",
    "cloud_2022q1.loc[:, 'cik'] = cloud_2022q1['cik'].astype(str).str.zfill(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0446465-3884-4ae6-b0ec-2dc963a3d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##N CORE TABLE \n",
    "\n",
    "table_1 =pd.merge(df, cloud_2022q1, on =['cik','quarter','year'], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b3b13-5df9-4824-b1c0-7d10eb29399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SEND TABLE UP TO BQ\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def upload_core_csv_to_bq_(df):\n",
    "    \"\"\"\n",
    "    Uploads parsed MDA data from EDGAR filings to BigQuery.\n",
    "    Expects columns: cik, filename, management_discussion\n",
    "    \"\"\"\n",
    "    # Rename columns to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Keep only the required columns\n",
    "\n",
    "        BQ_PROJECT_ID = 'sentiment-lewagon'\n",
    "        BQ_DATASET_ID = 'sentiment_db'\n",
    "        BQ_TABLE_ID = 'core'\n",
    "        table_ref = f\"{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_ID}\"\n",
    "\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=\"WRITE_APPEND\"\n",
    "        )\n",
    "\n",
    "        job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "        job.result()\n",
    "\n",
    "        print(f\"✅ Uploaded {job.output_rows} rows to {table_ref}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to upload DataFrame to BigQuery: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8000677-0846-43a2-a614-68529c1ee628",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UPLOAD TEST\n",
    "\n",
    "upload_core_csv_to_bq_(table_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
