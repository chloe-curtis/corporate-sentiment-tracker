{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66605318-2c4b-40ad-9570-ac413526ecfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T15:02:22.459125Z",
     "iopub.status.busy": "2025-06-04T15:02:22.458866Z",
     "iopub.status.idle": "2025-06-04T15:02:27.264344Z",
     "shell.execute_reply": "2025-06-04T15:02:27.263925Z",
     "shell.execute_reply.started": "2025-06-04T15:02:22.459096Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Use the notebook‐only tqdm (no attempt to import ipywidgets)\n",
    "from tqdm.notebook import tqdm   \n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download NLTK punkt (sentence splitter) if not already present\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "\n",
    "# ─── 2) Item 7 Extractor ─────────────────────────────────────────────────\n",
    "def extract_item7_from_10k(filepath: str, skip_chars: int = 18000) -> str:\n",
    "    \"\"\"\n",
    "    Reads a raw EDGAR 10-K text file and returns the Item 7 section\n",
    "    (“Management’s Discussion & Analysis…”) as a single string.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    text_to_search = raw_text[skip_chars:]\n",
    "    start_pattern = re.compile(r\"(?m)^ITEM\\s+7(?:\\s*[\\.\\-]|\\s).*\", re.IGNORECASE)\n",
    "    m_start = start_pattern.search(text_to_search)\n",
    "    if not m_start:\n",
    "        return None\n",
    "    start_idx = m_start.end()\n",
    "\n",
    "    end_pattern = re.compile(\n",
    "        r\"(?m)^(ITEM\\s+7A(?:\\s*[\\.\\-]|\\s).*|ITEM\\s+8(?:\\s*[\\.\\-]|\\s).*)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    m_end = end_pattern.search(text_to_search, pos=start_idx)\n",
    "    if not m_end:\n",
    "        return None\n",
    "    end_idx = m_end.start()\n",
    "\n",
    "    return text_to_search[start_idx:end_idx].strip()\n",
    "\n",
    "\n",
    "# ─── 3) Chunking Utility ────────────────────────────────────────────────\n",
    "def chunk_text_for_finbert(full_text: str, tokenizer, max_tokens: int = 510) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits `full_text` into strings of ≤ max_tokens tokens each,\n",
    "    by sentence‐splitting and accumulating until the limit is hit.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_len = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent_tokens = tokenizer.tokenize(sent)\n",
    "        sent_len = len(sent_tokens)\n",
    "        if current_len + sent_len > max_tokens and current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sent]\n",
    "            current_len = sent_len\n",
    "        else:\n",
    "            current_chunk.append(sent)\n",
    "            current_len += sent_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ─── 4) Main: Sentiment Function ─────────────────────────────────────────\n",
    "def get_sentiment_for(\n",
    "    ticker: str,\n",
    "    quarter: str,\n",
    "    raw_folder: str = \"raw_data/edgar_filings_2024_QTR4\",\n",
    "    model_name: str = \"ProsusAI/finbert\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    1) Finds the 10-K file for <ticker>_<quarter> in `raw_folder/`.\n",
    "    2) Extracts Item 7 (MD&A).\n",
    "    3) Splits it into ≤510-token chunks.\n",
    "    4) Runs FinBERT on each chunk.\n",
    "    5) Returns aggregated sentiment scores.\n",
    "    \"\"\"\n",
    "    # 4.1) Locate the 10-K file (e.g. “AAPL_2023Q3_10-K.txt”)\n",
    "    pattern = os.path.join(raw_folder, f\"{ticker}_*{quarter}*_10-K.txt\")\n",
    "    matches = glob.glob(pattern)\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No 10-K file found for ticker={ticker}, quarter={quarter} in {raw_folder}/\"\n",
    "        )\n",
    "    txt_path = matches[0]\n",
    "\n",
    "    # 4.2) Extract Item 7 text\n",
    "    item7_text = extract_item7_from_10k(txt_path, skip_chars=18000)\n",
    "    if not item7_text:\n",
    "        raise ValueError(f\"Could not extract MD&A (Item 7) from {txt_path}\")\n",
    "\n",
    "    # 4.3) Load FinBERT model + tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model     = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    nlp_pipe  = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # 4.4) Chunk MD&A into ≤510-token pieces\n",
    "    chunks = chunk_text_for_finbert(item7_text, tokenizer, max_tokens=510)\n",
    "\n",
    "    # 4.5) Run FinBERT on each chunk and collect scores\n",
    "    all_pos = []\n",
    "    all_neu = []\n",
    "    all_neg = []\n",
    "\n",
    "    for chunk in tqdm(chunks, desc=f\"Scoring {ticker} {quarter}\", leave=False):\n",
    "        out = nlp_pipe(chunk)\n",
    "        scores = {d[\"label\"].lower(): d[\"score\"] for d in out}\n",
    "        all_pos.append(scores.get(\"positive\", 0.0))\n",
    "        all_neu.append(scores.get(\"neutral\",  0.0))\n",
    "        all_neg.append(scores.get(\"negative\", 0.0))\n",
    "\n",
    "    # 4.6) Compute averages & net sentiment\n",
    "    avg_pos = sum(all_pos) / len(all_pos)\n",
    "    avg_neu = sum(all_neu) / len(all_neu)\n",
    "    avg_neg = sum(all_neg) / len(all_neg)\n",
    "    net_sent = avg_pos - avg_neg\n",
    "\n",
    "    result = {\n",
    "        \"ticker\":        ticker.upper(),\n",
    "        \"quarter\":       quarter.upper(),\n",
    "        \"num_chunks\":    len(chunks),\n",
    "        \"avg_positive\":  round(avg_pos,  4),\n",
    "        \"avg_neutral\":   round(avg_neu,   4),\n",
    "        \"avg_negative\":  round(avg_neg,   4),\n",
    "        \"net_sentiment\": round(net_sent,  4)\n",
    "    }\n",
    "\n",
    "    print(f\"\\n=== Sentiment for {ticker.upper()} {quarter.upper()} ===\")\n",
    "    print(f\"Chunks analyzed : {result['num_chunks']}\")\n",
    "    print(f\"Avg positive    : {result['avg_positive']}\")\n",
    "    print(f\"Avg neutral     : {result['avg_neutral']}\")\n",
    "    print(f\"Avg negative    : {result['avg_negative']}\")\n",
    "    print(f\"Net sentiment   : {result['net_sentiment']}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed4a6d20-774b-46ee-ae62-00f5634a03c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T15:26:45.006341Z",
     "iopub.status.busy": "2025-06-04T15:26:45.003947Z",
     "iopub.status.idle": "2025-06-04T15:26:45.677235Z",
     "shell.execute_reply": "2025-06-04T15:26:45.676648Z",
     "shell.execute_reply.started": "2025-06-04T15:26:45.006141Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No 10-K file found for ticker=APPLE, quarter= in raw_data/edgar_filings_2024_QTR4/",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m company_ticker = \u001b[33m\"\u001b[39m\u001b[33mAPPLE\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m company_quarter = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m sentiment_output = \u001b[43mget_sentiment_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany_ticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompany_quarter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mget_sentiment_for\u001b[39m\u001b[34m(ticker, quarter, raw_folder, model_name)\u001b[39m\n\u001b[32m     93\u001b[39m matches = glob.glob(pattern)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matches:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     96\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo 10-K file found for ticker=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, quarter=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquarter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     97\u001b[39m     )\n\u001b[32m     98\u001b[39m txt_path = matches[\u001b[32m0\u001b[39m]\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# 4.2) Extract Item 7 text\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No 10-K file found for ticker=APPLE, quarter= in raw_data/edgar_filings_2024_QTR4/"
     ]
    }
   ],
   "source": [
    "company_ticker = \"APPLE\"\n",
    "company_quarter = \"\"\n",
    "sentiment_output = get_sentiment_for(company_ticker, company_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2fe7bde-9cd5-4c96-9bb2-f7d20e8cd263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T15:19:02.066701Z",
     "iopub.status.busy": "2025-06-04T15:19:02.065061Z",
     "iopub.status.idle": "2025-06-04T15:19:05.199994Z",
     "shell.execute_reply": "2025-06-04T15:19:05.199507Z",
     "shell.execute_reply.started": "2025-06-04T15:19:02.066641Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/harshverma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 sentiment: [{'label': 'positive', 'score': 0.6094936728477478}]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Download punkt for sentence splitting (only need to run once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Your full MD&A text goes here. For example:\n",
    "text = \"\"\"\n",
    "Low interest rates lead to increased borrowing\n",
    "\"\"\"\n",
    "\n",
    "# Load FinBERT model and tokenizer\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Step 1: Split the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Step 2: Chunk sentences without exceeding the token limit\n",
    "max_len = 510  # Maximum number of tokens per chunk (leave room for special tokens)\n",
    "chunks = []\n",
    "current_chunk = []\n",
    "current_length = 0\n",
    "\n",
    "for sent in sentences:\n",
    "    sent_tokens = tokenizer.tokenize(sent)\n",
    "    sent_len = len(sent_tokens)\n",
    "\n",
    "    # If adding this sentence stays under max_len, append it\n",
    "    if current_length + sent_len <= max_len:\n",
    "        current_chunk.append(sent)\n",
    "        current_length += sent_len\n",
    "    else:\n",
    "        # Save the current chunk and start a new one\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk = [sent]\n",
    "        current_length = sent_len\n",
    "\n",
    "# Add the final chunk (if any)\n",
    "if current_chunk:\n",
    "    chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "# Step 3: Run FinBERT sentiment analysis on each chunk\n",
    "results = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    result = nlp(chunk)\n",
    "    print(f\"Chunk {i+1} sentiment:\", result)\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cf464-10e1-47ff-8fed-0a7bb0226aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
