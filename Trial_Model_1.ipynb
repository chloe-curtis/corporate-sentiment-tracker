{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2fe7bde-9cd5-4c96-9bb2-f7d20e8cd263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T15:19:02.066701Z",
     "iopub.status.busy": "2025-06-04T15:19:02.065061Z",
     "iopub.status.idle": "2025-06-04T15:19:05.199994Z",
     "shell.execute_reply": "2025-06-04T15:19:05.199507Z",
     "shell.execute_reply.started": "2025-06-04T15:19:02.066641Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/harshverma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 sentiment: [{'label': 'positive', 'score': 0.6094936728477478}]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Download punkt for sentence splitting (only need to run once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Your full MD&A text goes here. For example:\n",
    "text = \"\"\"\n",
    "Low interest rates lead to increased borrowing\n",
    "\"\"\"\n",
    "\n",
    "# Load FinBERT model and tokenizer\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Step 1: Split the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Step 2: Chunk sentences without exceeding the token limit\n",
    "max_len = 510  # Maximum number of tokens per chunk (leave room for special tokens)\n",
    "chunks = []\n",
    "current_chunk = []\n",
    "current_length = 0\n",
    "\n",
    "for sent in sentences:\n",
    "    sent_tokens = tokenizer.tokenize(sent)\n",
    "    sent_len = len(sent_tokens)\n",
    "\n",
    "    # If adding this sentence stays under max_len, append it\n",
    "    if current_length + sent_len <= max_len:\n",
    "        current_chunk.append(sent)\n",
    "        current_length += sent_len\n",
    "    else:\n",
    "        # Save the current chunk and start a new one\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk = [sent]\n",
    "        current_length = sent_len\n",
    "\n",
    "# Add the final chunk (if any)\n",
    "if current_chunk:\n",
    "    chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "# Step 3: Run FinBERT sentiment analysis on each chunk\n",
    "results = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    result = nlp(chunk)\n",
    "    print(f\"Chunk {i+1} sentiment:\", result)\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a450b-3ff5-4268-bf92-1ed513f3faa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T10:02:04.281284Z",
     "iopub.status.busy": "2025-06-05T10:02:04.281071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 406 total rows; 372 rows appear to be 10-Q (filename contains '_10-Q_').\n",
      "  → SKIPPING (empty/short MD&A) for: CIK=827052, filename=20241029_10-Q_edgar_data_827052_0000827052-24-000075.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Load FinBERT model + tokenizer\n",
    "# -----------------------------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Load your MD&A CSV: item2_extracted_sec.csv\n",
    "#    It must have at least these three columns:\n",
    "#       - 'cik'\n",
    "#       - 'filename'\n",
    "#       - 'item2'   (the full MD&A text)\n",
    "#\n",
    "#    If it has extra columns (date, exchange, etc.) that's fine; we ignore those.\n",
    "# -----------------------------------------------------------------------------\n",
    "path_mda = \"item2_extracted_sec.csv\"\n",
    "df_mda   = pd.read_csv(path_mda, dtype=str)\n",
    "\n",
    "# Sanity check: do we see those columns?\n",
    "required_cols = {\"cik\", \"filename\", \"item2\"}\n",
    "missing = required_cols - set(df_mda.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing columns in {path_mda}: {missing}\")\n",
    "\n",
    "# If some rows are 10-K instead of 10-Q, skip them:\n",
    "# (We assume all 10-Q filenames contain the substring \"_10-Q_\")\n",
    "df_mda_10q = df_mda[df_mda[\"filename\"].str.contains(\"_10-Q_\")].copy().reset_index(drop=True)\n",
    "print(f\"Loaded {len(df_mda)} total rows; {len(df_mda_10q)} rows appear to be 10-Q (filename contains '_10-Q_').\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Iterate row by row, run FinBERT on each 'item2' (MD&A) field\n",
    "# -----------------------------------------------------------------------------\n",
    "results = []\n",
    "for idx, row in df_mda_10q.iterrows():\n",
    "    cik      = row[\"cik\"]\n",
    "    filename = row[\"filename\"]\n",
    "    mda_text = row[\"item2\"]\n",
    "\n",
    "    if not isinstance(mda_text, str) or mda_text.strip() == \"\" or len(mda_text.strip()) < 20:\n",
    "        # If MD&A is empty or ridiculously short, skip\n",
    "        print(f\"  → SKIPPING (empty/short MD&A) for: CIK={cik}, filename={filename}\")\n",
    "        continue\n",
    "\n",
    "    # Split the MD&A into “paragraphs” by blank lines, and drop any piece < 50 chars\n",
    "    paragraphs = [\n",
    "        para.strip()\n",
    "        for para in re.split(r\"\\n+\", mda_text)\n",
    "        if len(para.strip()) > 50\n",
    "    ]\n",
    "\n",
    "    # If you prefer sentence‐splitting or sliding windows, swap this out; this is just a simple example.\n",
    "    chunk_sentiments = []\n",
    "    for para in paragraphs:\n",
    "        encoded = tokenizer(\n",
    "            para,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            out   = model(**encoded)\n",
    "            probs = softmax(out.logits, dim=1).squeeze().tolist()\n",
    "            # ProsusAI/finbert’s logits order is [positive, negative, neutral]\n",
    "            chunk_sentiments.append({\n",
    "                \"positive\": probs[0],\n",
    "                \"negative\": probs[1],\n",
    "                \"neutral\":  probs[2],\n",
    "            })\n",
    "\n",
    "    if chunk_sentiments:\n",
    "        avg_pos = sum(d[\"positive\"] for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "        avg_neg = sum(d[\"negative\"] for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "        avg_neu = sum(d[\"neutral\"]  for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "    else:\n",
    "        # If every “paragraph” was < 50 characters, we mark None (or 0) as you wish\n",
    "        avg_pos = avg_neg = avg_neu = None\n",
    "\n",
    "    results.append({\n",
    "        \"cik\": cik,\n",
    "        \"filename\": filename,\n",
    "        \"num_chunks\": len(chunk_sentiments),\n",
    "        \"avg_positive\": avg_pos,\n",
    "        \"avg_negative\": avg_neg,\n",
    "        \"avg_neutral\":  avg_neu\n",
    "    })\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Dump final sentiment scores to CSV\n",
    "# -----------------------------------------------------------------------------\n",
    "df_out = pd.DataFrame(results)\n",
    "df_out.to_csv(\"item2_sentiment_10Q.csv\", index=False)\n",
    "print(\"Done. Wrote 'item2_sentiment_10Q.csv' with\", len(df_out), \"rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f80474e-7651-404a-b7c0-702fe8ad9007",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T11:33:49.286581Z",
     "iopub.status.busy": "2025-06-05T11:33:49.276833Z",
     "iopub.status.idle": "2025-06-05T12:32:05.130611Z",
     "shell.execute_reply": "2025-06-05T12:32:05.125958Z",
     "shell.execute_reply.started": "2025-06-05T11:33:49.286107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'item2_extracted_sec.csv' → 406 total rows, 372 rows appear to be 10-Q (filename contains '_10-Q_').\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=827052, filename=20241029_10-Q_edgar_data_827052_0000827052-24-000075.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=1002047, filename=20241125_10-Q_edgar_data_1002047_0000950170-24-130551.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=1037868, filename=20241031_10-Q_edgar_data_1037868_0001037868-24-000053.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=831001, filename=20241107_10-Q_edgar_data_831001_0000831001-24-000134.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=895421, filename=20241104_10-Q_edgar_data_895421_0000895421-24-000491.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=878927, filename=20241106_10-Q_edgar_data_878927_0000950170-24-122440.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=65984, filename=20241101_10-Q_edgar_data_65984_0000065984-24-000112.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=723531, filename=20241219_10-Q_edgar_data_723531_0000950170-24-138425.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=1390777, filename=20241101_10-Q_edgar_data_1390777_0001390777-24-000133.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=1413329, filename=20241024_10-Q_edgar_data_1413329_0001413329-24-000172.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=35527, filename=20241105_10-Q_edgar_data_35527_0000035527-24-000249.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=896878, filename=20241121_10-Q_edgar_data_896878_0000896878-24-000053.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=1682852, filename=20241107_10-Q_edgar_data_1682852_0001682852-24-000060.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=899051, filename=20241030_10-Q_edgar_data_899051_0000899051-24-000083.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=713676, filename=20241101_10-Q_edgar_data_713676_0000713676-24-000081.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=1800, filename=20241031_10-Q_edgar_data_1800_0001628280-24-044602.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=4962, filename=20241018_10-Q_edgar_data_4962_0000004962-24-000068.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=1020569, filename=20241106_10-Q_edgar_data_1020569_0001020569-24-000255.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=815556, filename=20241016_10-Q_edgar_data_815556_0000815556-24-000037.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=1286681, filename=20241010_10-Q_edgar_data_1286681_0000950170-24-113947.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=1179929, filename=20241024_10-Q_edgar_data_1179929_0001179929-24-000151.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=874766, filename=20241024_10-Q_edgar_data_874766_0000874766-24-000122.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=51143, filename=20241030_10-Q_edgar_data_51143_0000051143-24-000049.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=62996, filename=20241029_10-Q_edgar_data_62996_0000062996-24-000033.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=49196, filename=20241029_10-Q_edgar_data_49196_0000049196-24-000095.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=2012383, filename=20241106_10-Q_edgar_data_2012383_0000950170-24-122256.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=916076, filename=20241030_10-Q_edgar_data_916076_0000950170-24-118821.txt\n",
      "  → SKIPPING (empty/short Management’s Discussion) for: CIK=55785, filename=20241022_10-Q_edgar_data_55785_0000055785-24-000094.txt\n",
      "Done. Wrote 'management_discussion_sentiment_10Q.csv' with 344 rows.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Load FinBERT model + tokenizer\n",
    "# -----------------------------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Load your CSV of pre-extracted Management’s Discussion and Analysis\n",
    "#\n",
    "#    - The CSV must have at least these three columns:\n",
    "#        • 'cik'\n",
    "#        • 'filename'  (must contain \"_10-Q_\" for actual 10-Q filings)\n",
    "#        • 'item2'     (the full text of Management’s Discussion and Analysis of Financial Condition and Results of Operations)\n",
    "#\n",
    "#    If there are extra columns, they will be ignored.\n",
    "# -----------------------------------------------------------------------------\n",
    "path_mda = \"item2_extracted_sec.csv\"\n",
    "df_all  = pd.read_csv(path_mda, dtype=str)\n",
    "\n",
    "# Check for required columns\n",
    "required_cols = {\"cik\", \"filename\", \"item2\"}\n",
    "missing_cols  = required_cols - set(df_all.columns)\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"Missing required columns in '{path_mda}': {missing_cols}\")\n",
    "\n",
    "# Keep only rows whose filename indicates a 10-Q (i.e. contains \"_10-Q_\")\n",
    "df_10q = df_all[df_all[\"filename\"].str.contains(\"_10-Q_\")].copy().reset_index(drop=True)\n",
    "print(f\"Loaded '{path_mda}' → {len(df_all)} total rows, {len(df_10q)} rows appear to be 10-Q (filename contains '_10-Q_').\")\n",
    "\n",
    "# Rename 'item2' to 'management_discussion' for clarity:\n",
    "df_10q = df_10q.rename(columns={\"item2\": \"management_discussion\"})\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Run FinBERT on each row’s Management’s Discussion and Analysis\n",
    "# -----------------------------------------------------------------------------\n",
    "results = []\n",
    "\n",
    "for idx, row in df_10q.iterrows():\n",
    "    cik      = row[\"cik\"]\n",
    "    filename = row[\"filename\"]\n",
    "    text_mda = row[\"management_discussion\"]\n",
    "\n",
    "    # Skip any empty or extremely short text\n",
    "    if not isinstance(text_mda, str) or text_mda.strip() == \"\" or len(text_mda.strip()) < 20:\n",
    "        print(f\"  → SKIPPING (empty/short Management’s Discussion) for: CIK={cik}, filename={filename}\")\n",
    "        continue\n",
    "\n",
    "    # Split the Management’s Discussion and Analysis into “paragraphs” by blank lines,\n",
    "    # dropping any short chunk (< 50 characters)\n",
    "    paragraphs = [\n",
    "        para.strip()\n",
    "        for para in re.split(r\"\\n+\", text_mda)\n",
    "        if len(para.strip()) > 50\n",
    "    ]\n",
    "\n",
    "    chunk_sentiments = []\n",
    "    for para in paragraphs:\n",
    "        encoded = tokenizer(\n",
    "            para,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            out   = model(**encoded)\n",
    "            probs = softmax(out.logits, dim=1).squeeze().tolist()\n",
    "            # FinBERT’s output ordering is [positive, negative, neutral]\n",
    "            chunk_sentiments.append({\n",
    "                \"positive\": probs[0],\n",
    "                \"negative\": probs[1],\n",
    "                \"neutral\":  probs[2],\n",
    "            })\n",
    "\n",
    "    if chunk_sentiments:\n",
    "        avg_pos = sum(d[\"positive\"] for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "        avg_neg = sum(d[\"negative\"] for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "        avg_neu = sum(d[\"neutral\"]  for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "    else:\n",
    "        # If all “paragraphs” were shorter than 50 chars, leave None\n",
    "        avg_pos = avg_neg = avg_neu = None\n",
    "\n",
    "    results.append({\n",
    "        \"cik\": cik,\n",
    "        \"filename\": filename,\n",
    "        \"num_paragraphs\": len(chunk_sentiments),\n",
    "        \"avg_positive\": avg_pos,\n",
    "        \"avg_negative\": avg_neg,\n",
    "        \"avg_neutral\":  avg_neu\n",
    "    })\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Dump final sentiment scores to CSV\n",
    "# -----------------------------------------------------------------------------\n",
    "df_out = pd.DataFrame(results)\n",
    "df_out.to_csv(\"management_discussion_sentiment_10Q.csv\", index=False)\n",
    "print(f\"Done. Wrote 'management_discussion_sentiment_10Q.csv' with {len(df_out)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654874e-62c3-4e14-ae37-2651ba383d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
