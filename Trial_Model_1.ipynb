{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66605318-2c4b-40ad-9570-ac413526ecfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T15:02:22.459125Z",
     "iopub.status.busy": "2025-06-04T15:02:22.458866Z",
     "iopub.status.idle": "2025-06-04T15:02:27.264344Z",
     "shell.execute_reply": "2025-06-04T15:02:27.263925Z",
     "shell.execute_reply.started": "2025-06-04T15:02:22.459096Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Use the notebook‐only tqdm (no attempt to import ipywidgets)\n",
    "from tqdm.notebook import tqdm   \n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download NLTK punkt (sentence splitter) if not already present\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "\n",
    "# ─── 2) Item 7 Extractor ─────────────────────────────────────────────────\n",
    "def extract_item7_from_10k(filepath: str, skip_chars: int = 18000) -> str:\n",
    "    \"\"\"\n",
    "    Reads a raw EDGAR 10-K text file and returns the Item 7 section\n",
    "    (“Management’s Discussion & Analysis…”) as a single string.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    text_to_search = raw_text[skip_chars:]\n",
    "    start_pattern = re.compile(r\"(?m)^ITEM\\s+7(?:\\s*[\\.\\-]|\\s).*\", re.IGNORECASE)\n",
    "    m_start = start_pattern.search(text_to_search)\n",
    "    if not m_start:\n",
    "        return None\n",
    "    start_idx = m_start.end()\n",
    "\n",
    "    end_pattern = re.compile(\n",
    "        r\"(?m)^(ITEM\\s+7A(?:\\s*[\\.\\-]|\\s).*|ITEM\\s+8(?:\\s*[\\.\\-]|\\s).*)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    m_end = end_pattern.search(text_to_search, pos=start_idx)\n",
    "    if not m_end:\n",
    "        return None\n",
    "    end_idx = m_end.start()\n",
    "\n",
    "    return text_to_search[start_idx:end_idx].strip()\n",
    "\n",
    "\n",
    "# ─── 3) Chunking Utility ────────────────────────────────────────────────\n",
    "def chunk_text_for_finbert(full_text: str, tokenizer, max_tokens: int = 510) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits `full_text` into strings of ≤ max_tokens tokens each,\n",
    "    by sentence‐splitting and accumulating until the limit is hit.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_len = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent_tokens = tokenizer.tokenize(sent)\n",
    "        sent_len = len(sent_tokens)\n",
    "        if current_len + sent_len > max_tokens and current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sent]\n",
    "            current_len = sent_len\n",
    "        else:\n",
    "            current_chunk.append(sent)\n",
    "            current_len += sent_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ─── 4) Main: Sentiment Function ─────────────────────────────────────────\n",
    "def get_sentiment_for(\n",
    "    ticker: str,\n",
    "    quarter: str,\n",
    "    raw_folder: str = \"raw_data/edgar_filings_2024_QTR4\",\n",
    "    model_name: str = \"ProsusAI/finbert\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    1) Finds the 10-K file for <ticker>_<quarter> in `raw_folder/`.\n",
    "    2) Extracts Item 7 (MD&A).\n",
    "    3) Splits it into ≤510-token chunks.\n",
    "    4) Runs FinBERT on each chunk.\n",
    "    5) Returns aggregated sentiment scores.\n",
    "    \"\"\"\n",
    "    # 4.1) Locate the 10-K file (e.g. “AAPL_2023Q3_10-K.txt”)\n",
    "    pattern = os.path.join(raw_folder, f\"{ticker}_*{quarter}*_10-K.txt\")\n",
    "    matches = glob.glob(pattern)\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No 10-K file found for ticker={ticker}, quarter={quarter} in {raw_folder}/\"\n",
    "        )\n",
    "    txt_path = matches[0]\n",
    "\n",
    "    # 4.2) Extract Item 7 text\n",
    "    item7_text = extract_item7_from_10k(txt_path, skip_chars=18000)\n",
    "    if not item7_text:\n",
    "        raise ValueError(f\"Could not extract MD&A (Item 7) from {txt_path}\")\n",
    "\n",
    "    # 4.3) Load FinBERT model + tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model     = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    nlp_pipe  = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # 4.4) Chunk MD&A into ≤510-token pieces\n",
    "    chunks = chunk_text_for_finbert(item7_text, tokenizer, max_tokens=510)\n",
    "\n",
    "    # 4.5) Run FinBERT on each chunk and collect scores\n",
    "    all_pos = []\n",
    "    all_neu = []\n",
    "    all_neg = []\n",
    "\n",
    "    for chunk in tqdm(chunks, desc=f\"Scoring {ticker} {quarter}\", leave=False):\n",
    "        out = nlp_pipe(chunk)\n",
    "        scores = {d[\"label\"].lower(): d[\"score\"] for d in out}\n",
    "        all_pos.append(scores.get(\"positive\", 0.0))\n",
    "        all_neu.append(scores.get(\"neutral\",  0.0))\n",
    "        all_neg.append(scores.get(\"negative\", 0.0))\n",
    "\n",
    "    # 4.6) Compute averages & net sentiment\n",
    "    avg_pos = sum(all_pos) / len(all_pos)\n",
    "    avg_neu = sum(all_neu) / len(all_neu)\n",
    "    avg_neg = sum(all_neg) / len(all_neg)\n",
    "    net_sent = avg_pos - avg_neg\n",
    "\n",
    "    result = {\n",
    "        \"ticker\":        ticker.upper(),\n",
    "        \"quarter\":       quarter.upper(),\n",
    "        \"num_chunks\":    len(chunks),\n",
    "        \"avg_positive\":  round(avg_pos,  4),\n",
    "        \"avg_neutral\":   round(avg_neu,   4),\n",
    "        \"avg_negative\":  round(avg_neg,   4),\n",
    "        \"net_sentiment\": round(net_sent,  4)\n",
    "    }\n",
    "\n",
    "    print(f\"\\n=== Sentiment for {ticker.upper()} {quarter.upper()} ===\")\n",
    "    print(f\"Chunks analyzed : {result['num_chunks']}\")\n",
    "    print(f\"Avg positive    : {result['avg_positive']}\")\n",
    "    print(f\"Avg neutral     : {result['avg_neutral']}\")\n",
    "    print(f\"Avg negative    : {result['avg_negative']}\")\n",
    "    print(f\"Net sentiment   : {result['net_sentiment']}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed4a6d20-774b-46ee-ae62-00f5634a03c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T15:26:45.006341Z",
     "iopub.status.busy": "2025-06-04T15:26:45.003947Z",
     "iopub.status.idle": "2025-06-04T15:26:45.677235Z",
     "shell.execute_reply": "2025-06-04T15:26:45.676648Z",
     "shell.execute_reply.started": "2025-06-04T15:26:45.006141Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No 10-K file found for ticker=APPLE, quarter= in raw_data/edgar_filings_2024_QTR4/",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m company_ticker = \u001b[33m\"\u001b[39m\u001b[33mAPPLE\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m company_quarter = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m sentiment_output = \u001b[43mget_sentiment_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany_ticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompany_quarter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mget_sentiment_for\u001b[39m\u001b[34m(ticker, quarter, raw_folder, model_name)\u001b[39m\n\u001b[32m     93\u001b[39m matches = glob.glob(pattern)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matches:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     96\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo 10-K file found for ticker=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, quarter=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquarter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     97\u001b[39m     )\n\u001b[32m     98\u001b[39m txt_path = matches[\u001b[32m0\u001b[39m]\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# 4.2) Extract Item 7 text\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No 10-K file found for ticker=APPLE, quarter= in raw_data/edgar_filings_2024_QTR4/"
     ]
    }
   ],
   "source": [
    "company_ticker = \"APPLE\"\n",
    "company_quarter = \"\"\n",
    "sentiment_output = get_sentiment_for(company_ticker, company_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2fe7bde-9cd5-4c96-9bb2-f7d20e8cd263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T15:19:02.066701Z",
     "iopub.status.busy": "2025-06-04T15:19:02.065061Z",
     "iopub.status.idle": "2025-06-04T15:19:05.199994Z",
     "shell.execute_reply": "2025-06-04T15:19:05.199507Z",
     "shell.execute_reply.started": "2025-06-04T15:19:02.066641Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/harshverma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 sentiment: [{'label': 'positive', 'score': 0.6094936728477478}]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Download punkt for sentence splitting (only need to run once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Your full MD&A text goes here. For example:\n",
    "text = \"\"\"\n",
    "Low interest rates lead to increased borrowing\n",
    "\"\"\"\n",
    "\n",
    "# Load FinBERT model and tokenizer\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Step 1: Split the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Step 2: Chunk sentences without exceeding the token limit\n",
    "max_len = 510  # Maximum number of tokens per chunk (leave room for special tokens)\n",
    "chunks = []\n",
    "current_chunk = []\n",
    "current_length = 0\n",
    "\n",
    "for sent in sentences:\n",
    "    sent_tokens = tokenizer.tokenize(sent)\n",
    "    sent_len = len(sent_tokens)\n",
    "\n",
    "    # If adding this sentence stays under max_len, append it\n",
    "    if current_length + sent_len <= max_len:\n",
    "        current_chunk.append(sent)\n",
    "        current_length += sent_len\n",
    "    else:\n",
    "        # Save the current chunk and start a new one\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk = [sent]\n",
    "        current_length = sent_len\n",
    "\n",
    "# Add the final chunk (if any)\n",
    "if current_chunk:\n",
    "    chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "# Step 3: Run FinBERT sentiment analysis on each chunk\n",
    "results = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    result = nlp(chunk)\n",
    "    print(f\"Chunk {i+1} sentiment:\", result)\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "708688c8-3d9a-415f-bd80-2594d7e71d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T09:33:40.453801Z",
     "iopub.status.busy": "2025-06-05T09:33:40.452370Z",
     "iopub.status.idle": "2025-06-05T09:33:42.023821Z",
     "shell.execute_reply": "2025-06-05T09:33:42.023098Z",
     "shell.execute_reply.started": "2025-06-05T09:33:40.453751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded core_table_v2.csv → 48 rows with filing_type = 10-Q\n",
      "Loaded item2_extracted_sec.csv → 372 total rows with '_10-Q_' in filename\n",
      "48 out of 48 10-Q filings have no MD&A text in item2_extracted_sec.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'item2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/corporate-sentiment-tracker/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'item2'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     68\u001b[39m cik      = row[\u001b[33m\"\u001b[39m\u001b[33mcik\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     69\u001b[39m filename = row[\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m item2 = \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mitem2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item2, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m item2.strip() == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# Skip if no MD&A text\u001b[39;00m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  → SKIPPING (item2) for: CIK=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcik\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, filename=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/corporate-sentiment-tracker/lib/python3.12/site-packages/pandas/core/series.py:1121\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1124\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/corporate-sentiment-tracker/lib/python3.12/site-packages/pandas/core/series.py:1237\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1236\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1237\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/corporate-sentiment-tracker/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'item2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Load FinBERT model/tokenizer\n",
    "# -----------------------------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Load your core metadata: (cik, filing_type, filename), but keep only 10-Q\n",
    "# -----------------------------------------------------------------------------\n",
    "df_meta = pd.read_csv(\"core_table_v2.csv\", dtype=str)[\n",
    "    [\"cik\", \"filing_type\", \"filename\"]\n",
    "].copy()\n",
    "\n",
    "# Normalize filing_type (strip whitespace, uppercase)\n",
    "df_meta[\"filing_type\"] = df_meta[\"filing_type\"].str.strip().str.upper()\n",
    "\n",
    "# Keep only rows where filing_type == \"10-Q\"\n",
    "df_meta_10q = df_meta[df_meta[\"filing_type\"] == \"10-Q\"].reset_index(drop=True)\n",
    "print(f\"Loaded core_table_v2.csv → {len(df_meta_10q)} rows with filing_type = 10-Q\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Load the combined MD&A file (item2_extracted_sec.csv), which has both 10-Q and 10-K.\n",
    "#    We only want the 10-Q rows, so we filter on 'filename' containing \"_10-Q_\".\n",
    "#\n",
    "#    item2_extracted_sec.csv columns (example): [date, cik, filename, item2, …]\n",
    "# -----------------------------------------------------------------------------\n",
    "path_10q = \"item2_extracted_sec.csv\"\n",
    "if not os.path.exists(path_10q):\n",
    "    raise FileNotFoundError(f\"Could not find '{path_10q}' in working folder\")\n",
    "\n",
    "# Read in all rows\n",
    "df_item2 = pd.read_csv(path_10q, dtype=str)\n",
    "\n",
    "# Keep only those where filename contains \"_10-Q_\" (i.e. actual 10-Q filings).\n",
    "df_10q_only = df_item2[df_item2[\"filename\"].str.contains(\"_10-Q_\")].copy()\n",
    "\n",
    "# Now reduce to exactly the columns we need: cik, filename, and the MD&A text is in 'item2'\n",
    "df_10q_only = df_10q_only[[\"cik\", \"filename\", \"item2\"]].rename(columns={\"item2\": \"mda_text\"})\n",
    "print(f\"Loaded item2_extracted_sec.csv → {len(df_10q_only)} total rows with '_10-Q_' in filename\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Merge metadata + MD&A on (cik, filename) ONLY\n",
    "#    (We drop filing_type from the merge keys, because we've already filtered both sides to 10-Q.)\n",
    "# -----------------------------------------------------------------------------\n",
    "df_merged = pd.merge(\n",
    "    df_meta_10q[[\"cik\", \"filename\"]],  # core metadata, only 10-Q rows\n",
    "    df_10q_only,                       # pre-extracted MD&A for 10-Q\n",
    "    how=\"left\",\n",
    "    on=[\"cik\", \"filename\"]\n",
    ")\n",
    "\n",
    "missing_md = df_merged[\"mda_text\"].isna().sum()\n",
    "print(f\"{missing_md} out of {len(df_merged)} 10-Q filings have no MD&A text in item2_extracted_sec.csv\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Run FinBERT on each row’s mda_text\n",
    "# -----------------------------------------------------------------------------\n",
    "results = []\n",
    "for idx, row in df_merged.iterrows():\n",
    "    cik      = row[\"cik\"]\n",
    "    filename = row[\"filename\"]\n",
    "    item2 = row[\"item2\"]\n",
    "\n",
    "    if not isinstance(item2, str) or item2.strip() == \"\":\n",
    "        # Skip if no MD&A text\n",
    "        print(f\"  → SKIPPING (item2) for: CIK={cik}, filename={filename}\")\n",
    "        continue\n",
    "\n",
    "    # Split into “paragraphs” by blank lines (drop any line shorter than 50 chars)\n",
    "    paragraphs = [\n",
    "        para.strip()\n",
    "        for para in re.split(r\"\\n+\", mda_text)\n",
    "        if len(para.strip()) > 50\n",
    "    ]\n",
    "\n",
    "    chunk_sentiments = []\n",
    "    for para in paragraphs:\n",
    "        encoded = tokenizer(\n",
    "            para,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            out   = model(**encoded)\n",
    "            probs = softmax(out.logits, dim=1).squeeze().tolist()\n",
    "            # ProsusAI/finbert’s output order is [positive, negative, neutral]\n",
    "            chunk_sentiments.append({\n",
    "                \"positive\": probs[0],\n",
    "                \"negative\": probs[1],\n",
    "                \"neutral\":  probs[2],\n",
    "            })\n",
    "\n",
    "    if chunk_sentiments:\n",
    "        avg_pos = sum(d[\"positive\"] for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "        avg_neg = sum(d[\"negative\"] for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "        avg_neu = sum(d[\"neutral\"]  for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "    else:\n",
    "        # In case the MD&A text was present but every paragraph < 50 chars, mark as None\n",
    "        avg_pos = avg_neg = avg_neu = None\n",
    "\n",
    "    results.append({\n",
    "        \"cik\": cik,\n",
    "        \"filename\": filename,\n",
    "        \"num_chunks\": len(chunk_sentiments),\n",
    "        \"avg_positive\": avg_pos,\n",
    "        \"avg_negative\": avg_neg,\n",
    "        \"avg_neutral\":  avg_neu\n",
    "    })\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Save final sentiment scores to CSV\n",
    "# -----------------------------------------------------------------------------\n",
    "df_out = pd.DataFrame(results)\n",
    "df_out.to_csv(\"filings_sentiment_10Q.csv\", index=False)\n",
    "print(\"Done. Wrote 'filings_sentiment_10Q.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a450b-3ff5-4268-bf92-1ed513f3faa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T10:02:04.281284Z",
     "iopub.status.busy": "2025-06-05T10:02:04.281071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 406 total rows; 372 rows appear to be 10-Q (filename contains '_10-Q_').\n",
      "  → SKIPPING (empty/short MD&A) for: CIK=827052, filename=20241029_10-Q_edgar_data_827052_0000827052-24-000075.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Load FinBERT model + tokenizer\n",
    "# -----------------------------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Load your MD&A CSV: item2_extracted_sec.csv\n",
    "#    It must have at least these three columns:\n",
    "#       - 'cik'\n",
    "#       - 'filename'\n",
    "#       - 'item2'   (the full MD&A text)\n",
    "#\n",
    "#    If it has extra columns (date, exchange, etc.) that's fine; we ignore those.\n",
    "# -----------------------------------------------------------------------------\n",
    "path_mda = \"item2_extracted_sec.csv\"\n",
    "df_mda   = pd.read_csv(path_mda, dtype=str)\n",
    "\n",
    "# Sanity check: do we see those columns?\n",
    "required_cols = {\"cik\", \"filename\", \"item2\"}\n",
    "missing = required_cols - set(df_mda.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing columns in {path_mda}: {missing}\")\n",
    "\n",
    "# If some rows are 10-K instead of 10-Q, skip them:\n",
    "# (We assume all 10-Q filenames contain the substring \"_10-Q_\")\n",
    "df_mda_10q = df_mda[df_mda[\"filename\"].str.contains(\"_10-Q_\")].copy().reset_index(drop=True)\n",
    "print(f\"Loaded {len(df_mda)} total rows; {len(df_mda_10q)} rows appear to be 10-Q (filename contains '_10-Q_').\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Iterate row by row, run FinBERT on each 'item2' (MD&A) field\n",
    "# -----------------------------------------------------------------------------\n",
    "results = []\n",
    "for idx, row in df_mda_10q.iterrows():\n",
    "    cik      = row[\"cik\"]\n",
    "    filename = row[\"filename\"]\n",
    "    mda_text = row[\"item2\"]\n",
    "\n",
    "    if not isinstance(mda_text, str) or mda_text.strip() == \"\" or len(mda_text.strip()) < 20:\n",
    "        # If MD&A is empty or ridiculously short, skip\n",
    "        print(f\"  → SKIPPING (empty/short MD&A) for: CIK={cik}, filename={filename}\")\n",
    "        continue\n",
    "\n",
    "    # Split the MD&A into “paragraphs” by blank lines, and drop any piece < 50 chars\n",
    "    paragraphs = [\n",
    "        para.strip()\n",
    "        for para in re.split(r\"\\n+\", mda_text)\n",
    "        if len(para.strip()) > 50\n",
    "    ]\n",
    "\n",
    "    # If you prefer sentence‐splitting or sliding windows, swap this out; this is just a simple example.\n",
    "    chunk_sentiments = []\n",
    "    for para in paragraphs:\n",
    "        encoded = tokenizer(\n",
    "            para,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            out   = model(**encoded)\n",
    "            probs = softmax(out.logits, dim=1).squeeze().tolist()\n",
    "            # ProsusAI/finbert’s logits order is [positive, negative, neutral]\n",
    "            chunk_sentiments.append({\n",
    "                \"positive\": probs[0],\n",
    "                \"negative\": probs[1],\n",
    "                \"neutral\":  probs[2],\n",
    "            })\n",
    "\n",
    "    if chunk_sentiments:\n",
    "        avg_pos = sum(d[\"positive\"] for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "        avg_neg = sum(d[\"negative\"] for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "        avg_neu = sum(d[\"neutral\"]  for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "    else:\n",
    "        # If every “paragraph” was < 50 characters, we mark None (or 0) as you wish\n",
    "        avg_pos = avg_neg = avg_neu = None\n",
    "\n",
    "    results.append({\n",
    "        \"cik\": cik,\n",
    "        \"filename\": filename,\n",
    "        \"num_chunks\": len(chunk_sentiments),\n",
    "        \"avg_positive\": avg_pos,\n",
    "        \"avg_negative\": avg_neg,\n",
    "        \"avg_neutral\":  avg_neu\n",
    "    })\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Dump final sentiment scores to CSV\n",
    "# -----------------------------------------------------------------------------\n",
    "df_out = pd.DataFrame(results)\n",
    "df_out.to_csv(\"item2_sentiment_10Q.csv\", index=False)\n",
    "print(\"Done. Wrote 'item2_sentiment_10Q.csv' with\", len(df_out), \"rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805d96e0-ac65-49c6-a6fa-b237284c273b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T11:25:09.503167Z",
     "iopub.status.busy": "2025-06-05T11:25:09.501727Z",
     "iopub.status.idle": "2025-06-05T11:25:14.739151Z",
     "shell.execute_reply": "2025-06-05T11:25:14.738758Z",
     "shell.execute_reply.started": "2025-06-05T11:25:09.503099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded core_table_v2.csv → 48 rows with filing_type = 10-Q\n",
      "Loaded item2_extracted_sec.csv → 372 total rows with '_10-Q_' in filename\n",
      "48 out of 48 10-Q filings have no MD&A text in item2_extracted_sec.csv\n",
      "  → SKIPPING (no MD&A) for: CIK=0001090872, filename=20220303_10-Q_edgar_data_1090872_0001090872-22-000007.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000006281, filename=20220216_10-Q_edgar_data_6281_0000006281-22-000020.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000006951, filename=20220224_10-Q_edgar_data_6951_0000006951-22-000011.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001730168, filename=20220310_10-Q_edgar_data_1730168_0001730168-22-000029.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000014693, filename=20220303_10-Q_edgar_data_14693_0000014693-22-000022.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000711404, filename=20220304_10-Q_edgar_data_711404_0000711404-22-000015.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000016732, filename=20220309_10-Q_edgar_data_16732_0000016732-22-000014.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000900075, filename=20220303_10-Q_edgar_data_900075_0000900075-22-000015.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000858877, filename=20220222_10-Q_edgar_data_858877_0000858877-22-000004.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000315189, filename=20220224_10-Q_edgar_data_315189_0001558370-22-001900.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001744489, filename=20220209_10-Q_edgar_data_1744489_0001744489-22-000059.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001645590, filename=20220303_10-Q_edgar_data_1645590_0001645590-22-000016.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000047217, filename=20220307_10-Q_edgar_data_47217_0000047217-22-000013.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000048465, filename=20220308_10-Q_edgar_data_48465_0000048465-22-000013.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000896878, filename=20220302_10-Q_edgar_data_896878_0000896878-22-000010.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001601046, filename=20220301_10-Q_edgar_data_1601046_0001601046-22-000011.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001613103, filename=20220303_10-Q_edgar_data_1613103_0001613103-22-000011.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001002047, filename=20220302_10-Q_edgar_data_1002047_0000950170-22-002669.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001604778, filename=20220203_10-Q_edgar_data_1604778_0001604778-22-000010.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000829224, filename=20220201_10-Q_edgar_data_829224_0000829224-22-000012.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000091419, filename=20220301_10-Q_edgar_data_91419_0000091419-22-000017.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000883241, filename=20220218_10-Q_edgar_data_883241_0000883241-22-000003.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000096021, filename=20220209_10-Q_edgar_data_96021_0000096021-22-000036.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001260221, filename=20220208_10-Q_edgar_data_1260221_0001260221-22-000013.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001116132, filename=20220210_10-Q_edgar_data_1116132_0001116132-22-000007.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000100493, filename=20220207_10-Q_edgar_data_100493_0000100493-22-000016.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000103379, filename=20220128_10-Q_edgar_data_103379_0000103379-22-000003.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001467373, filename=20220317_10-Q_edgar_data_1467373_0001467373-22-000144.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000866787, filename=20220318_10-Q_edgar_data_866787_0001558370-22-003914.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000815097, filename=20220328_10-Q_edgar_data_815097_0000815097-22-000029.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000909832, filename=20220310_10-Q_edgar_data_909832_0000909832-22-000005.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001048911, filename=20220317_10-Q_edgar_data_1048911_0000950170-22-004086.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000040704, filename=20220323_10-Q_edgar_data_40704_0001193125-22-082733.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001341439, filename=20220311_10-Q_edgar_data_1341439_0001564590-22-009859.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001467373, filename=20220317_10-Q_edgar_data_1467373_0001467373-22-000144.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000866787, filename=20220318_10-Q_edgar_data_866787_0001558370-22-003914.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000815097, filename=20220328_10-Q_edgar_data_815097_0000815097-22-000029.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000909832, filename=20220310_10-Q_edgar_data_909832_0000909832-22-000005.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001048911, filename=20220317_10-Q_edgar_data_1048911_0000950170-22-004086.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000040704, filename=20220323_10-Q_edgar_data_40704_0001193125-22-082733.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001341439, filename=20220311_10-Q_edgar_data_1341439_0001564590-22-009859.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001467373, filename=20220317_10-Q_edgar_data_1467373_0001467373-22-000144.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000866787, filename=20220318_10-Q_edgar_data_866787_0001558370-22-003914.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000815097, filename=20220328_10-Q_edgar_data_815097_0000815097-22-000029.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000909832, filename=20220310_10-Q_edgar_data_909832_0000909832-22-000005.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001048911, filename=20220317_10-Q_edgar_data_1048911_0000950170-22-004086.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0000040704, filename=20220323_10-Q_edgar_data_40704_0001193125-22-082733.txt\n",
      "  → SKIPPING (no MD&A) for: CIK=0001341439, filename=20220311_10-Q_edgar_data_1341439_0001564590-22-009859.txt\n",
      "Done. Wrote 'filings_sentiment_10Q.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Load FinBERT model/tokenizer\n",
    "# -----------------------------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Load your core metadata: (cik, filing_type, filename), but keep only 10-Q\n",
    "# -----------------------------------------------------------------------------\n",
    "df_meta = pd.read_csv(\"core_table_v2.csv\", dtype=str)[\n",
    "    [\"cik\", \"filing_type\", \"filename\"]\n",
    "].copy()\n",
    "\n",
    "# Normalize filing_type (strip whitespace, uppercase)\n",
    "df_meta[\"filing_type\"] = df_meta[\"filing_type\"].str.strip().str.upper()\n",
    "\n",
    "# Keep only rows where filing_type == \"10-Q\"\n",
    "df_meta_10q = df_meta[df_meta[\"filing_type\"] == \"10-Q\"].reset_index(drop=True)\n",
    "print(f\"Loaded core_table_v2.csv → {len(df_meta_10q)} rows with filing_type = 10-Q\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Load the combined MD&A file (item2_extracted_sec.csv), which has both 10-Q and 10-K.\n",
    "#    We only want the 10-Q rows, so we filter on 'filename' containing \"_10-Q_\".\n",
    "#\n",
    "#    item2_extracted_sec.csv columns (example): [date, cik, filename, item2, …]\n",
    "# -----------------------------------------------------------------------------\n",
    "path_10q = \"item2_extracted_sec.csv\"\n",
    "if not os.path.exists(path_10q):\n",
    "    raise FileNotFoundError(f\"Could not find '{path_10q}' in working folder\")\n",
    "\n",
    "# Read in all rows\n",
    "df_all_mda = pd.read_csv(path_10q, dtype=str)\n",
    "\n",
    "# Keep only those where filename contains \"_10-Q_\" (i.e. actual 10-Q filings).\n",
    "df_10q_only = df_all_mda[df_all_mda[\"filename\"].str.contains(\"_10-Q_\")].copy()\n",
    "\n",
    "# Now reduce to exactly the columns we need: cik, filename, and the MD&A text is in 'item2'\n",
    "df_10q_only = df_10q_only[[\"cik\", \"filename\", \"item2\"]].rename(columns={\"item2\": \"mda_text\"})\n",
    "print(f\"Loaded item2_extracted_sec.csv → {len(df_10q_only)} total rows with '_10-Q_' in filename\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Merge metadata + MD&A on (cik, filename) ONLY\n",
    "#    (We drop filing_type from the merge keys, because we've already filtered both sides to 10-Q.)\n",
    "# -----------------------------------------------------------------------------\n",
    "df_merged = pd.merge(\n",
    "    df_meta_10q[[\"cik\", \"filename\"]],  # core metadata, only 10-Q rows\n",
    "    df_10q_only,                       # pre-extracted MD&A for 10-Q\n",
    "    how=\"left\",\n",
    "    on=[\"cik\", \"filename\"]\n",
    ")\n",
    "\n",
    "missing_md = df_merged[\"mda_text\"].isna().sum()\n",
    "print(f\"{missing_md} out of {len(df_merged)} 10-Q filings have no MD&A text in item2_extracted_sec.csv\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Run FinBERT on each row’s mda_text\n",
    "# -----------------------------------------------------------------------------\n",
    "results = []\n",
    "for idx, row in df_merged.iterrows():\n",
    "    cik      = row[\"cik\"]\n",
    "    filename = row[\"filename\"]\n",
    "    mda_text = row[\"mda_text\"]\n",
    "\n",
    "    if not isinstance(mda_text, str) or mda_text.strip() == \"\":\n",
    "        # Skip if no MD&A text\n",
    "        print(f\"  → SKIPPING (no MD&A) for: CIK={cik}, filename={filename}\")\n",
    "        continue\n",
    "\n",
    "    # Split into “paragraphs” by blank lines (drop any line shorter than 50 chars)\n",
    "    paragraphs = [\n",
    "        para.strip()\n",
    "        for para in re.split(r\"\\n+\", mda_text)\n",
    "        if len(para.strip()) > 50\n",
    "    ]\n",
    "\n",
    "    chunk_sentiments = []\n",
    "    for para in paragraphs:\n",
    "        encoded = tokenizer(\n",
    "            para,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            out   = model(**encoded)\n",
    "            probs = softmax(out.logits, dim=1).squeeze().tolist()\n",
    "            # ProsusAI/finbert’s output order is [positive, negative, neutral]\n",
    "            chunk_sentiments.append({\n",
    "                \"positive\": probs[0],\n",
    "                \"negative\": probs[1],\n",
    "                \"neutral\":  probs[2],\n",
    "            })\n",
    "\n",
    "    if chunk_sentiments:\n",
    "        avg_pos = sum(d[\"positive\"] for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "        avg_neg = sum(d[\"negative\"] for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "        avg_neu = sum(d[\"neutral\"]  for d in chunk_sentiments) / len(chunk_sentiments)\n",
    "    else:\n",
    "        # In case the MD&A text was present but every paragraph < 50 chars, mark as None\n",
    "        avg_pos = avg_neg = avg_neu = None\n",
    "\n",
    "    results.append({\n",
    "        \"cik\": cik,\n",
    "        \"filename\": filename,\n",
    "        \"num_chunks\": len(chunk_sentiments),\n",
    "        \"avg_positive\": avg_pos,\n",
    "        \"avg_negative\": avg_neg,\n",
    "        \"avg_neutral\":  avg_neu\n",
    "    })\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Save final sentiment scores to CSV\n",
    "# -----------------------------------------------------------------------------\n",
    "df_out = pd.DataFrame(results)\n",
    "df_out.to_csv(\"filings_sentiment_10Q.csv\", index=False)\n",
    "print(\"Done. Wrote 'filings_sentiment_10Q.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f80474e-7651-404a-b7c0-702fe8ad9007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
