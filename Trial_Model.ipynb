{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "931f857b-5380-414f-87a5-70d7f6e2c0f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T13:57:26.209702Z",
     "iopub.status.busy": "2025-06-04T13:57:26.207618Z",
     "iopub.status.idle": "2025-06-04T13:57:26.226264Z",
     "shell.execute_reply": "2025-06-04T13:57:26.225894Z",
     "shell.execute_reply.started": "2025-06-04T13:57:26.209621Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/harshverma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "# ─── 2) Item 7 Extractor ─────────────────────────────────────────────────\n",
    "def extract_item7_from_10k(filepath: str, skip_chars: int = 18000) -> str:\n",
    "    with open(raw_data/edgar_filings_2024_QTR4, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    text_to_search = raw_text[skip_chars:]\n",
    "    start_pattern = re.compile(r\"(?m)^ITEM\\s+7(?:\\s*[\\.\\-]|\\s).*\", re.IGNORECASE)\n",
    "    m_start = start_pattern.search(text_to_search)\n",
    "    if not m_start:\n",
    "        return None\n",
    "    start_idx = m_start.end()\n",
    "\n",
    "    end_pattern = re.compile(\n",
    "        r\"(?m)^(ITEM\\s+7A(?:\\s*[\\.\\-]|\\s).*|ITEM\\s+8(?:\\s*[\\.\\-]|\\s).*)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    m_end = end_pattern.search(text_to_search, pos=start_idx)\n",
    "    if not m_end:\n",
    "        return None\n",
    "    end_idx = m_end.start()\n",
    "\n",
    "    return text_to_search[start_idx:end_idx].strip()\n",
    "\n",
    "\n",
    "# ─── 3) Chunking Utility ────────────────────────────────────────────────\n",
    "def chunk_text_for_finbert(full_text: str, tokenizer, max_tokens: int = 510) -> list[str]:\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_len = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent_tokens = tokenizer.tokenize(sent)\n",
    "        sent_len = len(sent_tokens)\n",
    "        if current_len + sent_len > max_tokens and current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sent]\n",
    "            current_len = sent_len\n",
    "        else:\n",
    "            current_chunk.append(sent)\n",
    "            current_len += sent_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ─── 4) Main: Sentiment Function ─────────────────────────────────────────\n",
    "def get_sentiment_for(\n",
    "    ticker: str,\n",
    "    quarter: str,\n",
    "    raw_folder: str = \"raw_data/edgar_filings_2024_QTR4\",\n",
    "    model_name: str = \"ProsusAI/finbert\"\n",
    ") -> dict:\n",
    "    # 4.1) Find the matching 10-K file\n",
    "    pattern = os.path.join(raw_folder, f\"{ticker}_*{quarter}*_10-K.txt\")\n",
    "    matches = glob.glob(pattern)\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No 10-K file found for {ticker} {quarter} in {raw_folder}/\")\n",
    "    txt_path = matches[0]\n",
    "\n",
    "    # 4.2) Extract Item 7\n",
    "    item7_text = extract_item7_from_10k(txt_path, skip_chars=18000)\n",
    "    if not item7_text:\n",
    "        raise ValueError(f\"Could not extract MD&A (Item 7) from {txt_path}\")\n",
    "\n",
    "    # 4.3) Load FinBERT\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model     = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    nlp_pipe  = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # 4.4) Chunk MD&A\n",
    "    chunks = chunk_text_for_finbert(item7_text, tokenizer, max_tokens=510)\n",
    "\n",
    "    # 4.5) Run FinBERT on each chunk\n",
    "    all_pos = []\n",
    "    all_neu = []\n",
    "    all_neg = []\n",
    "\n",
    "    for chunk in tqdm(chunks, desc=f\"Scoring {ticker} {quarter}\"):\n",
    "        out = nlp_pipe(chunk)\n",
    "        scores = {d[\"label\"].lower(): d[\"score\"] for d in out}\n",
    "        all_pos.append(scores.get(\"positive\", 0.0))\n",
    "        all_neu.append(scores.get(\"neutral\",  0.0))\n",
    "        all_neg.append(scores.get(\"negative\", 0.0))\n",
    "\n",
    "    # 4.6) Aggregate\n",
    "    avg_pos = sum(all_pos) / len(all_pos)\n",
    "    avg_neu = sum(all_neu) / len(all_neu)\n",
    "    avg_neg = sum(all_neg) / len(all_neg)\n",
    "    net_sent = avg_pos - avg_neg\n",
    "\n",
    "    result = {\n",
    "        \"ticker\":        ticker.upper(),\n",
    "        \"quarter\":       quarter.upper(),\n",
    "        \"num_chunks\":    len(chunks),\n",
    "        \"avg_positive\":  round(avg_pos,  4),\n",
    "        \"avg_neutral\":   round(avg_neu,   4),\n",
    "        \"avg_negative\":  round(avg_neg,   4),\n",
    "        \"net_sentiment\": round(net_sent,  4)\n",
    "    }\n",
    "\n",
    "    print(f\"\\n=== Sentiment for {ticker.upper()} {quarter.upper()} ===\")\n",
    "    print(f\"Chunks analyzed : {len(chunks)}\")\n",
    "    print(f\"Avg positive    : {result['avg_positive']}\")\n",
    "    print(f\"Avg neutral     : {result['avg_neutral']}\")\n",
    "    print(f\"Avg negative    : {result['avg_negative']}\")\n",
    "    print(f\"Net sentiment   : {result['net_sentiment']}\")\n",
    "\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
