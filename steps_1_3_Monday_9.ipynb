{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50512c3e-0ccb-4f5d-a7f9-8c771df49b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea91db8d-de80-4d4a-a527-9781f253e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from step_2_ticker_converter import ticker_converter\n",
    "from step_3_cloud_clean_data import cloud_clean_data\n",
    "from step_1_historical_index import historical_data, get_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163fb188-0c2b-4cfc-b29d-6d4742dca9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## STEP 3 FUNCTION\n",
    "\n",
    "from google.cloud import storage\n",
    "import re\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "def cloud_clean_data_gcs(bucket_name, folder_prefix):\n",
    "    # Initialize client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # List blobs in the folder\n",
    "    blobs = bucket.list_blobs(prefix=folder_prefix)\n",
    "\n",
    "    rows = []\n",
    "    for blob in blobs:\n",
    "        filename = blob.name.split(\"/\")[-1]\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        if \"10-Q\" not in filename and \"10-K\" not in filename:\n",
    "            continue\n",
    "\n",
    "        # Extract CIK and filing type\n",
    "        cik_match = re.search(r'edgar_data_(\\d+)_', filename)\n",
    "        type_match = re.search(r'10-[QK]', filename)\n",
    "        cik = cik_match.group(1) if cik_match else None\n",
    "        filing_type = type_match.group(0) if type_match else None\n",
    "\n",
    "        # Read first part of the blob (up to 5000 bytes)\n",
    "        try:\n",
    "            content = blob.download_as_bytes(start=0, end=4999).decode('utf-8', errors='ignore')\n",
    "            period_match = re.search(r'CONFORMED PERIOD OF REPORT:\\s*(\\d{8})', content)\n",
    "            period = period_match.group(1) if period_match else None\n",
    "        except Exception as e:\n",
    "            period = None\n",
    "\n",
    "        rows.append({\n",
    "            'cik': cik,\n",
    "            'filing_type': filing_type,\n",
    "            'filename': filename,\n",
    "            'conformed_period_of_report': period,\n",
    "            'bucket_file_path': f\"gs://{bucket_name}/{blob.name}\"\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b6e712-693d-47f9-b714-707033241090",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"sentiment_chloe-curtis\"\n",
    "quarters = [f\"{year}q{q}\" for year in range(2019, 2025) for q in range(1, 5)]\n",
    "dataframes = []\n",
    "\n",
    "for quarter in quarters:\n",
    "    try:\n",
    "        print(f\"Processing {quarter}...\")\n",
    "        df = cloud_clean_data_gcs(bucket_name, f\"{quarter}/\")\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {quarter}: {e}\")\n",
    "######### CONCATS ALL QUARTERS ##############\n",
    "cloud_combined = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a74b6-4cc5-46ec-8c3c-4af4c55966cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########## CLEANS FUNCTION 3 ##################\n",
    "# Convert to datetime if not already\n",
    "cloud_combined['conformed_period_of_report'] = pd.to_datetime(cloud_combined['conformed_period_of_report'])\n",
    "\n",
    "# Create the 'year' column\n",
    "cloud_combined['year'] = cloud_combined['conformed_period_of_report'].dt.year\n",
    "\n",
    "# Create the 'quarter' column in the format 'Q1', 'Q2', etc.\n",
    "cloud_combined['quarter'] = 'Q' + cloud_combined['conformed_period_of_report'].dt.quarter.astype(str)\n",
    "cloud_combined.loc[:, 'cik'] = cloud_combined['cik'].astype(str).str.zfill(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4441db7-fc1f-4d05-9818-4c4c3b944af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### STEP 1\n",
    "file_path = \"historical_data.txt\"\n",
    "historical_index = historical_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b7e2a-04ac-4908-81a1-b37d18eab795",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### STEP 2\n",
    "ticker_path = \"ticker_converter.json\"\n",
    "ticker_conversion = ticker_converter(ticker_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e844a2bd-fff0-493c-a92c-df00723292a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## JOIN STEP 1 & 2 ###################\n",
    "base_index = pd.merge(historical_index, ticker_conversion, on = \"ticker\", how = \"left\")\n",
    "### JOIN CAUSED NAS - AS TICKER CONVERSION DIDNT HAVE COMPANY NAME AND CIK\n",
    "### REMOVE THE NAS FROM THE JOIN\n",
    "x = base_index.dropna()\n",
    "## CREATE DF WITH THE NAS\n",
    "y = base_index[base_index.isnull().any(axis=1)]\n",
    "## DROP THE COLUMNS WE DONT NEED FROM NA DF\n",
    "g = y[['quarter','ticker']]\n",
    "## CSV HAS THE DATA WE NEED\n",
    "missing_company = pd.read_csv('missing_companies.csv')\n",
    "### MERGE BACK TO HAVE THE CIX AND COMPANY NAME \n",
    "z = pd.merge(g, missing_company, on = \"ticker\", how = \"left\")\n",
    "### PAD THE CIK NUMBERS WITH ZEROS - 10 CHARCTERS\n",
    "z['cik'] = z['cik'].astype(str).str.zfill(10)\n",
    "## CONCAT BOTH DFS\n",
    "df = pd.concat([x, z], axis=0, ignore_index=True)\n",
    "# Split the 'quarter' column into two parts\n",
    "df[['quarter', 'year']] = df['quarter'].str.split('-', expand=True)\n",
    "\n",
    "# Convert 2-digit year to 4-digit\n",
    "df['year'] = df['year'].apply(lambda x: '20' + x if int(x) < 50 else '19' + x)\n",
    "df['year'] = df['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87938d3-7b20-4c60-b89b-38460de04a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_v2 =pd.merge(df, cloud_combined, on =['cik','quarter','year'], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e416c06-7a38-43b7-b6e8-0cc7ee2136ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SEND TABLE UP TO BQ\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def upload_core_csv_to_bq_(df):\n",
    "    \"\"\"\n",
    "    Uploads parsed MDA data from EDGAR filings to BigQuery.\n",
    "    Expects columns: cik, filename, management_discussion\n",
    "    \"\"\"\n",
    "    # Rename columns to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Keep only the required columns\n",
    "\n",
    "        BQ_PROJECT_ID = 'sentiment-lewagon'\n",
    "        BQ_DATASET_ID = 'sentiment_db'\n",
    "        BQ_TABLE_ID = 'core_v2'\n",
    "        table_ref = f\"{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_ID}\"\n",
    "\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=\"WRITE_APPEND\"\n",
    "        )\n",
    "\n",
    "        job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "        job.result()\n",
    "\n",
    "        print(f\"✅ Uploaded {job.output_rows} rows to {table_ref}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to upload DataFrame to BigQuery: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6814fb5b-27e4-4fa7-9a00-36afcd64da3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
